{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DefaultAzureCredential failed to retrieve a token from the included credentials.\n",
            "Attempted credentials:\n",
            "\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
            "Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot.this issue.\n",
            "\tManagedIdentityCredential: No token received.\n",
            "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i am here\n"
          ]
        }
      ],
      "source": [
        "import azure.ai.ml\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "\n",
        "try:\n",
        "    credential =  DefaultAzureCredential()\n",
        "    credential.get_token(\"https://management.azure.com/default\")\n",
        "except Exception as ex:\n",
        "    print(\"i am here\")\n",
        "    credential = InteractiveBrowserCredential()\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i had to use cli credential with --no browser parameter , cause the redirect is having issues.\n",
        "\n",
        "import azure.ai.ml\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import AzureCliCredential\n",
        "\n",
        "\n",
        "credential= AzureCliCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"31241946-ad24-4495-85e2-31248c60d164\",\n",
        "    resource_group_name=\"ariefinternal\",\n",
        "    workspace_name=\"ariefmlworkspace\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Register and upload the car dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading good_car_data.csv\u001b[32m (< 1 MB): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44.2k/44.2k [00:00<00:00, 1.69MB/s]\u001b[0m\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Data({'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'car', 'description': 'Good Car Data', 'tags': {}, 'properties': {}, 'id': '/subscriptions/31241946-ad24-4495-85e2-31248c60d164/resourceGroups/ariefinternal/providers/Microsoft.MachineLearningServices/workspaces/ariefmlworkspace/data/car/versions/1', 'base_path': './', 'creation_context': <azure.ai.ml._restclient.v2022_05_01.models._models_py3.SystemData object at 0x7f06a811e640>, 'serialize': <msrest.serialization.Serializer object at 0x7f06ab6cfa90>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/31241946-ad24-4495-85e2-31248c60d164/resourcegroups/ariefinternal/workspaces/ariefmlworkspace/datastores/workspaceblobstore/paths/LocalUpload/4a337dba56504a0bc17f6e0e49d1adb1/good_car_data.csv', 'referenced_uris': None})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#this uploads the file from the local store to the default blob store under the default container and then LocalUpload/4*******/good_car_data.csv\n",
        "#this gibberish is a problem with the 4***\n",
        "# and the studio shows this LocalUpload/4a337dba56504a0bc17f6e0e49d1adb1/good_car_data.csv \n",
        "\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "my_path = \"abfss://amex@ariefgen2.dfs.core.windows.net/data/mlcardata\"\n",
        "car_cleaned_data = Data(\n",
        "    path=\"../../data/good_car_data.csv\",\n",
        "    type=AssetTypes.URI_FILE,\n",
        "    description=\"Good Car Data\",\n",
        "    name=\"car\",\n",
        "    version='1'\n",
        ")\n",
        "\n",
        "ml_client.data.create_or_update(car_cleaned_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_18773/3537701328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#this registers where the location is shown.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmy_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"abfss://amex@ariefgen2.dfs.core.windows.net/data/mlcardata/good_car_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m car_cleaned_data = Data(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAssetTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURI_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Data' is not defined"
          ]
        }
      ],
      "source": [
        "#this registers where the location is shown.\n",
        "my_path = \"abfss://amex@ariefgen2.dfs.core.windows.net/data/mlcardata/good_car_data.csv\"\n",
        "car_cleaned_data = Data(\n",
        "    path=my_path,\n",
        "    type=AssetTypes.URI_FILE,\n",
        "    description=\"The Real car Data\",\n",
        "    name=\"goodcardata\",\n",
        "    version='1'\n",
        ")\n",
        "\n",
        "ml_client.data.create_or_update(car_cleaned_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "you already have a cluster named amlcluster, we will resue it\n",
            "AmlCompute with name amlcluster is created, the compute size is STANDARD_D2_V2\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "cpu_compute_target=\"amlcluster\"\n",
        "\n",
        "try:\n",
        "    cpu_cluster =  ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"you already have a cluster named {cpu_compute_target}, we will resue it\"\n",
        "    )\n",
        "except Exception:\n",
        "    print(\"creating a new compute target\")\n",
        "    cpu_cluster=AmlCompute(\n",
        "        name=\"amlcluster\",\n",
        "        type=\"aml-compute\",\n",
        "        size=\"STANDARD_D2_V2\",\n",
        "        min_instances=0,\n",
        "        max_instances=2,\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\"\n",
        "        )\n",
        "    cpu_cluster=ml_client.begin_create_or_update(cpu_cluster)\n",
        "\n",
        "print(\n",
        "    f\"AmlCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "dependencies_dir=\"../dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../dependencies/conda.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile {dependencies_dir}/conda.yml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scipy=1.7.1\n",
        "  - scikit-learn=0.24.2\n",
        "  - pandas>=1.1,<1.2\n",
        "  - seaborn=0.11.2\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - xlrd==2.0.1\n",
        "    - mlflow==1.26.0\n",
        "    - azureml-mlflow==1.41.0\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "custom_env_name = \"arief-scikit-learn\"\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"arief scikit learn environment\",\n",
        "    tags={\"scikit-learn\":\"0.24.2\",\"whose-environment\":\"arief_scikit_env\"},\n",
        "    conda_file=os.path.join(dependencies_dir,\"conda.yml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
        "    version=\"1.0.0\"\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Environment({'is_anonymous': False, 'auto_increment_version': False, 'name': 'arief-scikit-learn', 'description': 'arief scikit learn environment', 'tags': {'scikit-learn': '0.24.2', 'whose-environment': 'arief_scikit_env'}, 'properties': {}, 'id': None, 'base_path': './', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fdc5d1a6a30>, 'version': '1.0.0', 'latest_version': None, 'conda_file': OrderedDict([('name', 'model-env'), ('channels', ['conda-forge']), ('dependencies', ['python=3.8', 'numpy=1.21.2', 'pip=21.2.4', 'scipy=1.7.1', 'scikit-learn=0.24.2', 'pandas>=1.1,<1.2', OrderedDict([('pip', ['inference-schema[numpy-support]==1.3.0', 'xlrd==2.0.1', 'mlflow==1.26.0', 'azureml-mlflow==1.41.0'])])])]), 'image': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest', 'build': None, 'inference_config': None, 'os_type': None, 'arm_type': 'environment_version', 'conda_file_path': PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/arbavan1/code/Users/arbavan/azure-data-science/mlin100days/carprediction/azure/dependencies/conda.yml'), 'path': None, 'upload_hash': None, 'translated_conda_file': 'name: model-env\\nchannels:\\n- conda-forge\\ndependencies:\\n- python=3.8\\n- numpy=1.21.2\\n- pip=21.2.4\\n- scipy=1.7.1\\n- scikit-learn=0.24.2\\n- pandas>=1.1,<1.2\\n- pip:\\n  - inference-schema[numpy-support]==1.3.0\\n  - xlrd==2.0.1\\n  - mlflow==1.26.0\\n  - azureml-mlflow==1.41.0\\n'})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_job_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_job_env =  ml_client.environments.create_or_update(pipeline_job_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the first Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment with name arief-scikit-learn is registered to workspace, the environment version is 1.0.0\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "data_prep_src_dir=\"../components/data_prep\"\n",
        "os.makedirs(data_prep_src_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../components/data_prep/data_prep.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {data_prep_src_dir}/data_prep.py\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "import mlflow\n",
        "import seaborn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script\n",
        "    got it\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, help=\"split ratio\", required=False, default=0.2)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    #start logging\n",
        "    mlflow.start_run()\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "    print(\"input data:\",args.data)\n",
        "\n",
        "\n",
        "    car= pd.read_csv(args.data,header=1, index_col=0)\n",
        "    print(\"initial size data\", car.shape)\n",
        "    \n",
        "    #remove any price greater than 6000000\n",
        "    car=car[car['Price']<6000000]\n",
        "    print(\"Size after removing rows with price greater than 6000000 \", car.shape)\n",
        "     \n",
        "    mlflow.log_metric(\"initial_num_samples\", car.shape[0])\n",
        "    mlflow.log_metric(\"initial_num_features\", car.shape[1] - 1)\n",
        "\n",
        "    X=car[['name','company','year','kms_driven','fuel_type']]\n",
        "    y=car['Price'] \n",
        "\n",
        "    #train test split\n",
        "    X_train,X_test,y_train,y_test=train_test_split(car,y, test_size=args.test_train_ratio)\n",
        "   \n",
        "    #get all the categories from all the data\n",
        "    ohe=OneHotEncoder()\n",
        "    ohe.fit(car[['name','company','fuel_type']])\n",
        "\n",
        "    #make column transformer for all the categorical variables.\n",
        "    column_trans=make_column_transformer((OneHotEncoder(categories=ohe.categories_),['name','company','fuel_type']), remainder='passthrough')\n",
        "    # fit transform and convert from sparse array\n",
        "    X_train =column_trans.fit_transform(X_train).toarray()\n",
        "    X_test = column_trans.fit_transform(X_test).toarray()\n",
        "    \n",
        "    #convert to dataframe and merge with price, please convert the series into dataframe by using to_frame() function\n",
        "    train_df=pd.DataFrame(X_train)\n",
        "    train_df.merge(y_train.to_frame(), left_index=True, right_index=True)\n",
        "    test_df =pd.DataFrame(X_test)\n",
        "    test_df.merge(y_test.to_frame(), left_index=True, right_index=True)\n",
        "\n",
        "    #log the metrics\n",
        "    mlflow.log_metric(\"TRANSFORMED_TRAIN_num_samples\", train_df.shape[0])\n",
        "    mlflow.log_metric(\"TRANSFORMED_TRAIN_num_features\", train_df.shape[1] - 1)\n",
        "\n",
        "    mlflow.log_metric(\"TRANSFORMED_TEST_num_samples\", test_df.shape[0])\n",
        "    mlflow.log_metric(\"TRANSFORMED_TEST_num_features\", test_df.shape[1] - 1)\n",
        "\n",
        "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
        "    train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "    test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_prep_component =  command(\n",
        "    name=\"car_data_prep_components\",\n",
        "    display_name=\"Car Data Prepartion for training\",\n",
        "    description=\" reads a csv file, uses one hot encoder and breaks into training and test data\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_file\"),\n",
        "        \"test_train_ratio\": Input(type=\"number\"),\n",
        "    },\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    ),\n",
        "    code=data_prep_src_dir,\n",
        "    command=\"\"\" python data_prep.py \\\n",
        "        --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
        "        --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
        "    \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "train_src_dir = \"../components/train\"\n",
        "os.makedirs(train_src_dir,exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../components/train/train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {train_src_dir}/train.py\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_df =  pd.read_csv(select_first_file(args.train_data))\n",
        "    train_label=train_df[\"Price\"]\n",
        "    train_df=train_df.drop(columns=['Price'])\n",
        "    \n",
        "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
        "    test_label=test_df[\"Price\"]\n",
        "    test_df=test_df.drop(columns=['Price'])\n",
        "\n",
        "    #convert into array\n",
        "    X_train=train.df.values\n",
        "    y_train=train_label.values\n",
        "\n",
        "    X_test=test_df.values\n",
        "    y_test=test_label.values\n",
        "\n",
        "    #fit the model\n",
        "    lr=LinearRegression()\n",
        "    lr.fit(X_train,y_train)\n",
        "\n",
        "    y_pred=lr.predict(X_test)\n",
        "    print(r2_score(y_test,y_pred))\n",
        "    \n",
        "    #register the model to the workspace\n",
        "    print(\"registering the model\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=lr,\n",
        "        registered_model_name=args.registered_model_name,\n",
        "        artifact_path=args.registered_model_name,\n",
        "    )\n",
        "\n",
        "    #saving the model\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=lr,\n",
        "        path=os.path.join(args.model,\"trained_model\"),\n",
        "    )\n",
        "\n",
        "    #stop logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ../components/train/train.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile {train_src_dir}/train.yml\n",
        "\n",
        "name: train_car_prediction_model \n",
        "display_name: Train Car Prediction model \n",
        "type: command\n",
        "inputs:\n",
        "  train_data: \n",
        "    type: uri_folder\n",
        "  test_data: \n",
        "    type: uri_folder\n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  # for this step, we'll use an AzureML curate environment\n",
        "  azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\n",
        "command: >-\n",
        "  python train.py \n",
        "  --train_data ${{inputs.train_data}} \n",
        "  --test_data ${{inputs.test_data}} \n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import load_component\n",
        "train_component = load_component(path=os.path.join(train_src_dir,\"train.yml\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading train (0.0 MBs): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2857/2857 [00:00<00:00, 51865.80it/s]\u001b[0m\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Component train_car_prediction_model with Version 1 is registered\n"
          ]
        }
      ],
      "source": [
        "#register the component\n",
        "train_component=ml_client.create_or_update(train_component)\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create the pipeline from components\n",
        "\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target,\n",
        "    description=\"End To End Car Prediction Data Train Pipeline\",\n",
        ")\n",
        "def car_prediction_pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    #data prep\n",
        "    data_prep_job= data_prep_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    #using train_func like python call with its own input\n",
        "    train_job = train_component(\n",
        "        train_data=data_prep_job.outputs.train_data, #using outputs from previous step\n",
        "        test_data = data_prep_job.outputs.test_data, \n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    #a pipeline returns a dict of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "registered_model_name=\"campusx_car_prediction_model\"\n",
        "\n",
        "#lets instantiate the pipeline\n",
        "pipeline=  car_prediction_pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=my_path),\n",
        "    pipeline_job_test_train_ratio=0.2,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading data_prep (0.0 MBs): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3385/3385 [00:00<00:00, 36374.47it/s]\u001b[0m\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import webbrowser\n",
        "\n",
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"car_prediction_e2e_registered_components\",\n",
        ")\n",
        "# open the pipeline in web browser\n",
        "webbrowser.open(pipeline_job.services[\"Studio\"].endpoint)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
