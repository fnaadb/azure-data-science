from azureml.core.runconfig import RunConfiguration
from azureml.core.compute import AmlCompute
from azureml.core.conda_dependencies import CondaDependencies
from azureml.core import Experiment, Workspace, ScriptRunConfig, Datastore

import os




ws = Workspace.get(
       name='ariefmlworkspace',
       subscription_id='31241946-ad24-4495-85e2-31248c60d164',
        resource_group='ariefinternal')


""" 
#submitting an experiment, with environment configured with conda dependencies.

compute_config = RunConfiguration()
compute_config.target = "arief-cluster"
#compute_config.amlcompute.vm_size = "STANDARD_D1_V2"

dependencies = CondaDependencies()
dependencies.add_pip_package("scikit-learn")
dependencies.add_pip_package("numpy==1.15.4")
compute_config.environment.python.conda_dependencies = dependencies


script_run_config = ScriptRunConfig(source_directory=os.getcwd(), script="train.py", run_config=compute_config)
experiment = Experiment(workspace=ws, name="compute_target_test")
run = experiment.submit(config=script_run_config)
run.wait_for_completion(show_output=True)

"""

#upload data
def_blob_store = ws.get_default_datastore() 
# The following call GETS the Azure Blob Store associated with your workspace.
# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** 
def_blob_store = Datastore(ws, "workspaceblobstore")
print("Blobstore's name: {}".format(def_blob_store.name))

"""
A run represents a single trial of an experiment. A Run object is used to monitor the asynchronous execution of a trial, 
log metrics and store output of the trial, and to analyze results and access artifacts generated by the trial. 
handle to store and retrieve metrics, data, upload/download files, tags as well as child hierarchy

Loading the current run from a remote environment with the get_context method
Creating a run interactively in a notebook using start_logging
Logging metrics and uploading artifacts in your experiment, such as when using log
Reading metrics and downloading artifacts when analyzing experimental results, such as when using get_metrics

To submit a run, create a configuration object that describes how the experiment is run. Here are examples of the different configuration objects you can use:

ScriptRunConfig
     ScriptRunConfig packages together the configuration information needed to submit a run in Azure ML, including the script, compute target, environment, and any distributed job-specific configs.
     Once a script run is configured and submitted with the submit, a ScriptRun is returned.
    ScriptRunConfig(source_directory, script=None, arguments=None, run_config=None, _telemetry_values=None, compute_target=None, environment=None, distributed_job_config=None,
     resume_from=None, max_run_duration_seconds=2592000, command=None, docker_runtime_config=None)
AutoMLConfig

HyperDriveConfig

Pipeline

PublishedPipeline

PipelineEndpoint
"""


"""
azure ml provides a step, has inputs has arguments, parameters and outputs. The types:

PythonScriptStep: Adds a step to run a Python script in a Pipeline.
AdlaStep: Adds a step to run U-SQL script using Azure Data Lake Analytics.
DataTransferStep: Transfers data between Azure Blob and Data Lake accounts.
DatabricksStep: Adds a DataBricks notebook as a step in a Pipeline.
HyperDriveStep: Creates a Hyper Drive step for Hyper Parameter Tuning in a Pipeline.
AzureBatchStep: Creates a step for submitting jobs to Azure Batch
EstimatorStep: Adds a step to run Estimator in a Pipeline.
MpiStep: Adds a step to run a MPI job in a Pipeline.
AutoMLStep: Creates a AutoML step in a Pipeline.
"""

